{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3ccdc9a2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-01T23:05:31.342019Z",
     "iopub.status.busy": "2025-04-01T23:05:31.341751Z",
     "iopub.status.idle": "2025-04-01T23:05:37.841766Z",
     "shell.execute_reply": "2025-04-01T23:05:37.840990Z"
    },
    "papermill": {
     "duration": 6.505579,
     "end_time": "2025-04-01T23:05:37.843359",
     "exception": false,
     "start_time": "2025-04-01T23:05:31.337780",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Brain Tumor Segmentation for MRI on Kaggle\n",
    "import torch\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import nibabel as nib\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch import optim\n",
    "from sklearn.metrics import jaccard_score\n",
    "import optuna\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3f95ad84",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-01T23:05:37.849676Z",
     "iopub.status.busy": "2025-04-01T23:05:37.849307Z",
     "iopub.status.idle": "2025-04-01T23:05:37.854902Z",
     "shell.execute_reply": "2025-04-01T23:05:37.854289Z"
    },
    "papermill": {
     "duration": 0.009862,
     "end_time": "2025-04-01T23:05:37.856178",
     "exception": false,
     "start_time": "2025-04-01T23:05:37.846316",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# ==========================================\n",
    "# Dataset\n",
    "# ==========================================\n",
    "class BrainPatchesDataset(Dataset):\n",
    "    def __init__(self, data_dir_X, data_dir_y, transform=None):\n",
    "        self.X_paths = sorted(list(Path(data_dir_X).glob(\"*.npz\")))\n",
    "        self.y_paths = sorted(list(Path(data_dir_y).glob(\"*.npz\")))\n",
    "        assert len(self.X_paths) == len(self.y_paths), \"Mismatch in number of X and y patches\"\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.X_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Wczytanie danych z plików .npz\n",
    "        X_npz = np.load(self.X_paths[idx])\n",
    "        y_npz = np.load(self.y_paths[idx])\n",
    "\n",
    "        X = X_npz[\"X\"].astype(np.float32)\n",
    "        y = y_npz[\"y\"].astype(np.float32)\n",
    "\n",
    "        # Zamiana na tensory\n",
    "        X_tensor = torch.from_numpy(X)\n",
    "        y_tensor = torch.from_numpy(y)\n",
    "\n",
    "        # Dodaj kanał jeśli trzeba (np. [C, H, W, D])\n",
    "        if X_tensor.ndim == 3:\n",
    "            X_tensor = X_tensor.unsqueeze(0)\n",
    "        if y_tensor.ndim == 3:\n",
    "            y_tensor = y_tensor.unsqueeze(0)\n",
    "\n",
    "        if self.transform:\n",
    "            X_tensor, y_tensor = self.transform(X_tensor, y_tensor)\n",
    "\n",
    "        return X_tensor, y_tensor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4b530f13",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-01T23:05:37.861782Z",
     "iopub.status.busy": "2025-04-01T23:05:37.861586Z",
     "iopub.status.idle": "2025-04-01T23:05:37.872820Z",
     "shell.execute_reply": "2025-04-01T23:05:37.872239Z"
    },
    "papermill": {
     "duration": 0.015235,
     "end_time": "2025-04-01T23:05:37.873923",
     "exception": false,
     "start_time": "2025-04-01T23:05:37.858688",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# ==========================================\n",
    "# Model\n",
    "# ==========================================\n",
    "\n",
    "class DoubleConv(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super().__init__()\n",
    "        self.double_conv = nn.Sequential(\n",
    "            nn.Conv3d(in_channels, out_channels, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm3d(out_channels),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv3d(out_channels, out_channels, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm3d(out_channels),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.double_conv(x)\n",
    "\n",
    "class AttentionGate(nn.Module):\n",
    "    def __init__(self, F_g, F_l, F_int):\n",
    "        super().__init__()\n",
    "        self.W_g = nn.Sequential(\n",
    "            nn.Conv3d(F_g, F_int, kernel_size=1, stride=1, padding=0, bias=True),\n",
    "            nn.BatchNorm3d(F_int)\n",
    "        )\n",
    "\n",
    "        self.W_x = nn.Sequential(\n",
    "            nn.Conv3d(F_l, F_int, kernel_size=1, stride=1, padding=0, bias=True),\n",
    "            nn.BatchNorm3d(F_int)\n",
    "        )\n",
    "\n",
    "        self.psi = nn.Sequential(\n",
    "            nn.Conv3d(F_int, 1, kernel_size=1, stride=1, padding=0, bias=True),\n",
    "            nn.BatchNorm3d(1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "\n",
    "    def forward(self, g, x):\n",
    "        g1 = self.W_g(g)\n",
    "        x1 = self.W_x(x)\n",
    "        psi = self.relu(g1 + x1)\n",
    "        psi = self.psi(psi)\n",
    "        return x * psi\n",
    "\n",
    "# ======================================\n",
    "# === Attention U-Net 3D ===\n",
    "# ======================================\n",
    "\n",
    "class AttentionUNet3D(nn.Module):\n",
    "    def __init__(self, in_channels=4, out_channels=3, base_channels=32):\n",
    "        super().__init__()\n",
    "\n",
    "        self.enc1 = DoubleConv(in_channels, base_channels)\n",
    "        self.pool1 = nn.MaxPool3d(2)\n",
    "\n",
    "        self.enc2 = DoubleConv(base_channels, base_channels*2)\n",
    "        self.pool2 = nn.MaxPool3d(2)\n",
    "\n",
    "        self.enc3 = DoubleConv(base_channels*2, base_channels*4)\n",
    "        self.pool3 = nn.MaxPool3d(2)\n",
    "\n",
    "        self.bottleneck = DoubleConv(base_channels*4, base_channels*8)\n",
    "\n",
    "        self.up3 = nn.ConvTranspose3d(base_channels*8, base_channels*4, kernel_size=2, stride=2)\n",
    "        self.att3 = AttentionGate(F_g=base_channels*4, F_l=base_channels*4, F_int=base_channels*2)\n",
    "        self.dec3 = DoubleConv(base_channels*8, base_channels*4)\n",
    "\n",
    "        self.up2 = nn.ConvTranspose3d(base_channels*4, base_channels*2, kernel_size=2, stride=2)\n",
    "        self.att2 = AttentionGate(F_g=base_channels*2, F_l=base_channels*2, F_int=base_channels)\n",
    "        self.dec2 = DoubleConv(base_channels*4, base_channels*2)\n",
    "\n",
    "        self.up1 = nn.ConvTranspose3d(base_channels*2, base_channels, kernel_size=2, stride=2)\n",
    "        self.att1 = AttentionGate(F_g=base_channels, F_l=base_channels, F_int=base_channels//2)\n",
    "        self.dec1 = DoubleConv(base_channels*2, base_channels)\n",
    "\n",
    "        self.out_conv = nn.Conv3d(base_channels, out_channels, kernel_size=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        enc1 = self.enc1(x)\n",
    "        enc2 = self.enc2(self.pool1(enc1))\n",
    "        enc3 = self.enc3(self.pool2(enc2))\n",
    "        bottleneck = self.bottleneck(self.pool3(enc3))\n",
    "\n",
    "        up3 = self.up3(bottleneck)\n",
    "        att3 = self.att3(up3, enc3)\n",
    "        dec3 = self.dec3(torch.cat([up3, att3], dim=1))\n",
    "\n",
    "        up2 = self.up2(dec3)\n",
    "        att2 = self.att2(up2, enc2)\n",
    "        dec2 = self.dec2(torch.cat([up2, att2], dim=1))\n",
    "\n",
    "        up1 = self.up1(dec2)\n",
    "        att1 = self.att1(up1, enc1)\n",
    "        dec1 = self.dec1(torch.cat([up1, att1], dim=1))\n",
    "\n",
    "        return self.out_conv(dec1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "43393fa4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-01T23:05:37.879498Z",
     "iopub.status.busy": "2025-04-01T23:05:37.879221Z",
     "iopub.status.idle": "2025-04-01T23:05:37.885360Z",
     "shell.execute_reply": "2025-04-01T23:05:37.883945Z"
    },
    "papermill": {
     "duration": 0.010572,
     "end_time": "2025-04-01T23:05:37.886906",
     "exception": false,
     "start_time": "2025-04-01T23:05:37.876334",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# ==========================================\n",
    "# Loss function\n",
    "# ==========================================\n",
    "class DiceLoss(nn.Module):\n",
    "    def __init__(self, smooth=1e-5):\n",
    "        super().__init__()\n",
    "        self.smooth = smooth\n",
    "\n",
    "    def forward(self, preds, targets):\n",
    "        preds = torch.sigmoid(preds)\n",
    "        assert preds.shape == targets.shape, \"Shape mismatch between preds and targets\"\n",
    "\n",
    "        intersection = (preds * targets).sum(dim=(2, 3, 4))\n",
    "        union = preds.sum(dim=(2, 3, 4)) + targets.sum(dim=(2, 3, 4))\n",
    "        dice = (2. * intersection + self.smooth) / (union + self.smooth)\n",
    "\n",
    "        return 1 - dice.mean()  # Średnia po klasach i batchu\n",
    "\n",
    "class CombinedLoss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.dice = DiceLoss()\n",
    "        self.bce = nn.BCEWithLogitsLoss()\n",
    "\n",
    "    def forward(self, preds, targets):\n",
    "        return 0.5 * self.dice(preds, targets) + 0.5 * self.bce(preds, targets)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "90ad1df3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-01T23:05:37.892349Z",
     "iopub.status.busy": "2025-04-01T23:05:37.892082Z",
     "iopub.status.idle": "2025-04-01T23:05:37.900785Z",
     "shell.execute_reply": "2025-04-01T23:05:37.900181Z"
    },
    "papermill": {
     "duration": 0.012691,
     "end_time": "2025-04-01T23:05:37.902000",
     "exception": false,
     "start_time": "2025-04-01T23:05:37.889309",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# ==========================================\n",
    "# Evaluation\n",
    "# ==========================================\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from sklearn.metrics import recall_score\n",
    "\n",
    "def dice_score(pred, target):\n",
    "    smooth = 1e-5\n",
    "    intersection = (pred * target).sum()\n",
    "    union = pred.sum() + target.sum()\n",
    "    return (2 * intersection + smooth) / (union + smooth)\n",
    "\n",
    "def evaluate_model_multiclass(model, val_loader, device):\n",
    "    model.eval()\n",
    "\n",
    "    dice = {\"WT\": [], \"TC\": [], \"EC\": []}\n",
    "    sensitivity = {\"WT\": [], \"TC\": [], \"EC\": []}\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for imgs, masks in val_loader:\n",
    "            imgs, masks = imgs.to(device), masks.to(device)\n",
    "\n",
    "            outputs = model(imgs)  # (B, C=3, D, H, W)\n",
    "            probs = torch.sigmoid(outputs)\n",
    "            preds = (probs > 0.5).float()\n",
    "\n",
    "            for i in range(imgs.shape[0]):\n",
    "                pred = preds[i].cpu().numpy()\n",
    "                target = masks[i].cpu().numpy()\n",
    "\n",
    "                # === Binarize categories ===\n",
    "                pred_WT = (pred.sum(axis=0) > 0).astype(int)          # Whole tumor = any of class 1-3\n",
    "                pred_TC = ((pred[0] + pred[2]) > 0).astype(int)       # TC = class 1 (necrosis) + 3 (enhancing)\n",
    "                pred_EC = (pred[2] > 0).astype(int)                   # EC = class 3 (enhancing)\n",
    "\n",
    "                true_WT = (target.sum(axis=0) > 0).astype(int)\n",
    "                true_TC = ((target[0] + target[2]) > 0).astype(int)\n",
    "                true_EC = (target[2] > 0).astype(int)\n",
    "\n",
    "                # === Dice ===\n",
    "                dice[\"WT\"].append(dice_score(pred_WT, true_WT))\n",
    "                dice[\"TC\"].append(dice_score(pred_TC, true_TC))\n",
    "                dice[\"EC\"].append(dice_score(pred_EC, true_EC))\n",
    "\n",
    "                # === Sensitivity ===\n",
    "                for label, pred_bin, true_bin in zip([\"WT\", \"TC\", \"EC\"],\n",
    "                                                     [pred_WT, pred_TC, pred_EC],\n",
    "                                                     [true_WT, true_TC, true_EC]):\n",
    "                    if true_bin.sum() == 0:\n",
    "                        sensitivity[label].append(np.nan)\n",
    "                    else:\n",
    "                        recall = recall_score(true_bin.flatten(), pred_bin.flatten(), zero_division=0)\n",
    "                        sensitivity[label].append(recall)\n",
    "\n",
    "    # Final results\n",
    "    results = {\n",
    "        \"dice\": {k: np.nanmean(v) for k, v in dice.items()},\n",
    "        \"sensitivity\": {k: np.nanmean(v) for k, v in sensitivity.items()}\n",
    "    }\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "152d3134",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-01T23:05:37.907595Z",
     "iopub.status.busy": "2025-04-01T23:05:37.907397Z",
     "iopub.status.idle": "2025-04-01T23:05:51.000078Z",
     "shell.execute_reply": "2025-04-01T23:05:50.999132Z"
    },
    "papermill": {
     "duration": 13.097338,
     "end_time": "2025-04-01T23:05:51.001766",
     "exception": false,
     "start_time": "2025-04-01T23:05:37.904428",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# ==========================================\n",
    "# Training\n",
    "# ==========================================\n",
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from torch.nn.utils import clip_grad_norm_\n",
    "\n",
    "class EarlyStopping:\n",
    "    def __init__(self, patience=10, min_delta=0.001):\n",
    "        self.patience = patience\n",
    "        self.min_delta = min_delta\n",
    "        self.best_score = None\n",
    "        self.counter = 0\n",
    "        self.early_stop = False\n",
    "\n",
    "    def __call__(self, current_score):\n",
    "        if self.best_score is None or current_score > self.best_score + self.min_delta:\n",
    "            self.best_score = current_score\n",
    "            self.counter = 0\n",
    "        else:\n",
    "            self.counter += 1\n",
    "            if self.counter >= self.patience:\n",
    "                self.early_stop = True\n",
    "\n",
    "\n",
    "def train_model(model, train_loader, val_loader, criterion, optimizer, device, epochs=10, save_path=\"/kaggle/working/results\"):\n",
    "    os.makedirs(save_path, exist_ok=True)\n",
    "    writer = SummaryWriter(log_dir=os.path.join(save_path, \"logs\"))\n",
    "\n",
    "    train_losses, val_dices = [], []\n",
    "    best_dice = 0.0\n",
    "    early_stopping = EarlyStopping(patience=10, min_delta=0.001)\n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', patience=3)\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        epoch_loss = 0\n",
    "\n",
    "        for imgs, masks in train_loader:\n",
    "            imgs = imgs.to(device)\n",
    "            masks = masks.to(device)\n",
    "\n",
    "            if imgs.dim() == 4:\n",
    "                imgs = imgs.unsqueeze(1)\n",
    "            if masks.dim() == 4:\n",
    "                masks = masks.unsqueeze(1)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(imgs)\n",
    "\n",
    "            if masks.shape != outputs.shape:\n",
    "                masks = F.interpolate(masks, size=outputs.shape[2:], mode='nearest')\n",
    "\n",
    "            loss = criterion(outputs, masks)\n",
    "            loss.backward()\n",
    "            clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "            optimizer.step()\n",
    "            epoch_loss += loss.item()\n",
    "\n",
    "        avg_loss = epoch_loss / len(train_loader)\n",
    "        train_losses.append(avg_loss)\n",
    "\n",
    "        metrics = evaluate_model_multiclass(model, val_loader, device)\n",
    "        val_dices.append(metrics[\"dice\"][\"WT\"])\n",
    "        val_sens = metrics[\"sensitivity\"][\"WT\"]\n",
    "\n",
    "        scheduler.step(avg_loss)\n",
    "\n",
    "        writer.add_scalar(\"Loss/train\", avg_loss, epoch)\n",
    "        writer.add_scalar(\"Val/Dice_WT\", metrics[\"dice\"][\"WT\"], epoch)\n",
    "        writer.add_scalar(\"Val/Sens_WT\", val_sens, epoch)\n",
    "\n",
    "        for cls in [\"WT\", \"TC\", \"EC\"]:\n",
    "            writer.add_scalar(f\"Val/Dice_{cls}\", metrics[\"dice\"][cls], epoch)\n",
    "            writer.add_scalar(f\"Val/Sens_{cls}\", metrics[\"sensitivity\"][cls], epoch)\n",
    "\n",
    "        print(f\"Epoch {epoch+1}/{epochs} - Train Loss: {avg_loss:.4f} - Dice WT: {metrics['dice']['WT']:.4f} - Sens WT: {val_sens:.4f}\")\n",
    "\n",
    "        current_dice = metrics[\"dice\"][\"WT\"]\n",
    "        if current_dice > best_dice:\n",
    "            best_dice = current_dice\n",
    "            torch.save(model.state_dict(), os.path.join(save_path, \"best_model.pth\"))\n",
    "            writer.add_text(\"Model\", f\"Best model saved at epoch {epoch+1} (Dice WT: {current_dice:.4f})\", epoch)\n",
    "            print(f\"✅ Best model saved (Dice WT: {current_dice:.4f})\")\n",
    "\n",
    "        # ✅ Zapis metryk i ostatniego modelu co epokę\n",
    "        np.save(os.path.join(save_path, \"train_loss.npy\"), train_losses)\n",
    "        np.save(os.path.join(save_path, \"val_dice.npy\"), val_dices)\n",
    "        torch.save(model.state_dict(), os.path.join(save_path, \"last_model.pth\"))\n",
    "        writer.flush()\n",
    "\n",
    "        early_stopping(current_dice)\n",
    "        if early_stopping.early_stop:\n",
    "            writer.add_text(\"EarlyStopping\", f\"Triggered at epoch {epoch+1}\", epoch)\n",
    "            print(f\"⛔ Early stopping triggered at epoch {epoch+1}\")\n",
    "            break\n",
    "\n",
    "    writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "67cd1d45",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-01T23:05:51.007866Z",
     "iopub.status.busy": "2025-04-01T23:05:51.007388Z",
     "iopub.status.idle": "2025-04-01T23:05:51.116117Z",
     "shell.execute_reply": "2025-04-01T23:05:51.115143Z"
    },
    "papermill": {
     "duration": 0.112999,
     "end_time": "2025-04-01T23:05:51.117489",
     "exception": false,
     "start_time": "2025-04-01T23:05:51.004490",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Devices: 2 Tesla T4\n"
     ]
    }
   ],
   "source": [
    "print(\"Devices:\", torch.cuda.device_count(), torch.cuda.get_device_name(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "387a22a2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-01T23:05:51.123444Z",
     "iopub.status.busy": "2025-04-01T23:05:51.123166Z",
     "iopub.status.idle": "2025-04-01T23:05:51.217914Z",
     "shell.execute_reply": "2025-04-01T23:05:51.217296Z"
    },
    "papermill": {
     "duration": 0.099029,
     "end_time": "2025-04-01T23:05:51.219067",
     "exception": false,
     "start_time": "2025-04-01T23:05:51.120038",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# ==========================================\n",
    "# Run Training\n",
    "# ==========================================\n",
    "data_dir = Path(\"/kaggle/input/braintumor-dataset-patches\")\n",
    "\n",
    "train_dataset = BrainPatchesDataset(data_dir / \"train\" / \"images\", data_dir / \"train\" / \"masks\")\n",
    "val_dataset = BrainPatchesDataset(data_dir / \"val\" / \"images\", data_dir / \"val\" / \"masks\")\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=4, shuffle=True, num_workers=2, pin_memory=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=4, shuffle=False, num_workers=2, pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cab07a8a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-01T23:05:51.224747Z",
     "iopub.status.busy": "2025-04-01T23:05:51.224498Z",
     "iopub.status.idle": "2025-04-02T03:43:29.144307Z",
     "shell.execute_reply": "2025-04-02T03:43:29.143073Z"
    },
    "papermill": {
     "duration": 16657.9293,
     "end_time": "2025-04-02T03:43:29.150869",
     "exception": false,
     "start_time": "2025-04-01T23:05:51.221569",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Number of GPUs available: 2\n",
      "Using 2 GPUs with DataParallel.\n",
      "Epoch 1/80 - Train Loss: 0.4171 - Dice WT: 0.8495 - Sens WT: 0.8569\n",
      "✅ Best model saved (Dice WT: 0.8495)\n",
      "Epoch 2/80 - Train Loss: 0.2658 - Dice WT: 0.8486 - Sens WT: 0.8389\n",
      "Epoch 3/80 - Train Loss: 0.2511 - Dice WT: 0.8196 - Sens WT: 0.7722\n",
      "Epoch 4/80 - Train Loss: 0.2441 - Dice WT: 0.8446 - Sens WT: 0.8751\n",
      "Epoch 5/80 - Train Loss: 0.2391 - Dice WT: 0.8622 - Sens WT: 0.8826\n",
      "✅ Best model saved (Dice WT: 0.8622)\n",
      "Epoch 6/80 - Train Loss: 0.2350 - Dice WT: 0.8654 - Sens WT: 0.8489\n",
      "✅ Best model saved (Dice WT: 0.8654)\n",
      "Epoch 7/80 - Train Loss: 0.2319 - Dice WT: 0.8682 - Sens WT: 0.9055\n",
      "✅ Best model saved (Dice WT: 0.8682)\n",
      "Epoch 8/80 - Train Loss: 0.2278 - Dice WT: 0.8711 - Sens WT: 0.8837\n",
      "✅ Best model saved (Dice WT: 0.8711)\n",
      "Epoch 9/80 - Train Loss: 0.2258 - Dice WT: 0.8563 - Sens WT: 0.8279\n",
      "Epoch 10/80 - Train Loss: 0.2234 - Dice WT: 0.8597 - Sens WT: 0.8359\n",
      "Epoch 11/80 - Train Loss: 0.2193 - Dice WT: 0.8698 - Sens WT: 0.8506\n",
      "Epoch 12/80 - Train Loss: 0.2184 - Dice WT: 0.8691 - Sens WT: 0.8920\n",
      "Epoch 13/80 - Train Loss: 0.2161 - Dice WT: 0.8600 - Sens WT: 0.8444\n",
      "Epoch 14/80 - Train Loss: 0.2148 - Dice WT: 0.8796 - Sens WT: 0.8961\n",
      "✅ Best model saved (Dice WT: 0.8796)\n",
      "Epoch 15/80 - Train Loss: 0.2134 - Dice WT: 0.8817 - Sens WT: 0.8841\n",
      "✅ Best model saved (Dice WT: 0.8817)\n",
      "Epoch 16/80 - Train Loss: 0.2114 - Dice WT: 0.8617 - Sens WT: 0.8203\n",
      "Epoch 17/80 - Train Loss: 0.2098 - Dice WT: 0.8694 - Sens WT: 0.8528\n",
      "Epoch 18/80 - Train Loss: 0.2082 - Dice WT: 0.8738 - Sens WT: 0.9203\n",
      "Epoch 19/80 - Train Loss: 0.2071 - Dice WT: 0.8837 - Sens WT: 0.8872\n",
      "✅ Best model saved (Dice WT: 0.8837)\n",
      "Epoch 20/80 - Train Loss: 0.2051 - Dice WT: 0.8834 - Sens WT: 0.8780\n",
      "Epoch 21/80 - Train Loss: 0.2044 - Dice WT: 0.8780 - Sens WT: 0.9117\n",
      "Epoch 22/80 - Train Loss: 0.2036 - Dice WT: 0.8798 - Sens WT: 0.8686\n",
      "Epoch 23/80 - Train Loss: 0.2031 - Dice WT: 0.8854 - Sens WT: 0.8996\n",
      "✅ Best model saved (Dice WT: 0.8854)\n",
      "Epoch 24/80 - Train Loss: 0.2024 - Dice WT: 0.8804 - Sens WT: 0.8502\n",
      "Epoch 25/80 - Train Loss: 0.2003 - Dice WT: 0.8827 - Sens WT: 0.8888\n",
      "Epoch 26/80 - Train Loss: 0.2003 - Dice WT: 0.8855 - Sens WT: 0.8845\n",
      "✅ Best model saved (Dice WT: 0.8855)\n",
      "Epoch 27/80 - Train Loss: 0.2001 - Dice WT: 0.8729 - Sens WT: 0.8499\n",
      "Epoch 28/80 - Train Loss: 0.1995 - Dice WT: 0.8902 - Sens WT: 0.8779\n",
      "✅ Best model saved (Dice WT: 0.8902)\n",
      "Epoch 29/80 - Train Loss: 0.1975 - Dice WT: 0.8852 - Sens WT: 0.8727\n",
      "Epoch 30/80 - Train Loss: 0.1986 - Dice WT: 0.8851 - Sens WT: 0.9060\n",
      "Epoch 31/80 - Train Loss: 0.1965 - Dice WT: 0.8882 - Sens WT: 0.9069\n",
      "Epoch 32/80 - Train Loss: 0.1952 - Dice WT: 0.8840 - Sens WT: 0.9163\n",
      "Epoch 33/80 - Train Loss: 0.1954 - Dice WT: 0.8895 - Sens WT: 0.9119\n",
      "Epoch 34/80 - Train Loss: 0.1949 - Dice WT: 0.8911 - Sens WT: 0.8816\n",
      "✅ Best model saved (Dice WT: 0.8911)\n",
      "Epoch 35/80 - Train Loss: 0.1923 - Dice WT: 0.8871 - Sens WT: 0.9308\n",
      "Epoch 36/80 - Train Loss: 0.1940 - Dice WT: 0.8800 - Sens WT: 0.8441\n",
      "Epoch 37/80 - Train Loss: 0.1924 - Dice WT: 0.8885 - Sens WT: 0.9128\n",
      "Epoch 38/80 - Train Loss: 0.1905 - Dice WT: 0.8899 - Sens WT: 0.8780\n",
      "⛔ Early stopping triggered at epoch 38\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "print(f\"Number of GPUs available: {torch.cuda.device_count()}\")\n",
    "\n",
    "# Model initialization\n",
    "model = AttentionUNet3D(in_channels=4, out_channels=3)\n",
    "\n",
    "# Wrap with DataParallel if multiple GPUs\n",
    "if torch.cuda.device_count() > 1:\n",
    "    print(f\"Using {torch.cuda.device_count()} GPUs with DataParallel.\")\n",
    "    model = torch.nn.DataParallel(model)\n",
    "\n",
    "model = model.to(device)\n",
    "\n",
    "\n",
    "#device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "#model = AttentionUNet3D(in_channels=4, out_channels=3).to(device)\n",
    "\n",
    "criterion = CombinedLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-3, weight_decay=1e-5)\n",
    "\n",
    "train_model(\n",
    "    model=model,\n",
    "    train_loader=train_loader,\n",
    "    val_loader=val_loader,\n",
    "    criterion=criterion,\n",
    "    optimizer=optimizer,\n",
    "    device=device,\n",
    "    epochs=80,\n",
    "    save_path=\"/kaggle/working/results/attention_unet\"\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "datasetId": 7024024,
     "sourceId": 11242224,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30919,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 16684.138446,
   "end_time": "2025-04-02T03:43:32.675718",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-04-01T23:05:28.537272",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
